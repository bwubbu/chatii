#!/bin/bash
set -e

# Start Ollama in background
echo "üöÄ Starting Ollama server..."
ollama serve &
OLLAMA_PID=$!

# Wait for Ollama to be ready (Cloud Run needs this to be reliable)
echo "‚è≥ Waiting for Ollama to start..."
for i in {1..30}; do
  if curl -s http://localhost:11434/api/tags > /dev/null 2>&1; then
    echo "‚úÖ Ollama is ready!"
    break
  fi
  if [ $i -eq 30 ]; then
    echo "‚ùå Ollama failed to start after 30 seconds"
    exit 1
  fi
  sleep 1
done

# Pull the model (use environment variable or default to 8B model)
MODEL=${MODEL:-llama3.1:8b-instruct-q4_0}
echo "üì• Pulling model $MODEL..."
ollama pull $MODEL || {
  echo "‚ö†Ô∏è  Model pull failed, trying fallback..."
  ollama pull llama3.1:8b-instruct-q4_0 || echo "‚ùå Failed to pull model"
}

# Verify Ollama is still running
if ! kill -0 $OLLAMA_PID 2>/dev/null; then
  echo "‚ùå Ollama process died!"
  exit 1
fi

# Start FastAPI server (Cloud Run sets PORT automatically)
PORT=${PORT:-8080}
echo "üåê Starting FastAPI server on port $PORT with model $MODEL..."
exec python ollama_chatbot_server.py --host 0.0.0.0 --port $PORT --model $MODEL
