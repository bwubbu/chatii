@echo off
echo ğŸš€ Starting Fairness Chatbot with Ollama Integration
echo.

REM Check if Ollama is installed
where ollama >nul 2>nul
if %ERRORLEVEL% NEQ 0 (
    echo âŒ Ollama is not installed or not in PATH
    echo Please install Ollama from https://ollama.com/
    pause
    exit /b 1
)

REM Check if the required model is available
echo ğŸ“¦ Checking for Llama 3.1 model...
ollama list | findstr /i "llama3.1:8b-instruct-q4_0" >nul
if %ERRORLEVEL% NEQ 0 (
    echo ğŸ“¥ Downloading Llama 3.1 model (this may take a few minutes)...
    ollama pull llama3.1:8b-instruct-q4_0
    if %ERRORLEVEL% NEQ 0 (
        echo âŒ Failed to download model
        pause
        exit /b 1
    )
)

echo âœ… Model ready!
echo.

REM Start Ollama server in background
echo ğŸ”§ Starting Ollama server...
start /b ollama serve >nul 2>nul

REM Wait for Ollama to start
timeout /t 3 /nobreak >nul

REM Start FastAPI server in new window
echo ğŸ¤– Starting FastAPI server...
start "Fairness ChatBot API" cmd /k "python ollama_chatbot_server.py --model llama3.1:8b-instruct-q4_0 --port 8000"

REM Wait for FastAPI server to start
echo â³ Waiting for FastAPI server to start...
timeout /t 5 /nobreak >nul

REM Start Next.js development server in new window
echo ğŸŒ Starting Next.js development server...
start "Next.js Dev Server" cmd /k "npm run dev"

echo.
echo ğŸ‰ All servers are starting up!
echo.
echo ğŸ“ What's running:
echo   â€¢ Ollama Server: Background process
echo   â€¢ FastAPI Server: http://localhost:8000 (new window)
echo   â€¢ Next.js Server: http://localhost:3000 (new window)
echo.
echo ğŸ“š FastAPI Docs: http://localhost:8000/docs
echo ğŸŒ Chat App: http://localhost:3000
echo.
echo â³ Please wait ~30 seconds for all servers to fully start...
echo Then open http://localhost:3000 in your browser
echo.
echo ğŸ’¡ To test the integration, run: python test_integration.py
echo.
pause 