{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "# 🤗 Upload Trained Model to Hugging Face Hub\n",
        "\n",
        "This notebook helps you upload your trained Mistral fairness model to Hugging Face Hub, where you can use the Inference API for cloud-based inference without running servers.\n",
        "\n",
        "**Benefits:**\n",
        "- No server management required\n",
        "- Automatic scaling\n",
        "- Built-in caching and optimization\n",
        "- Simple API calls\n",
        "- Free tier available\n",
        "\n",
        "**Steps:**\n",
        "1. Create Hugging Face account and get API token\n",
        "2. Upload your trained model\n",
        "3. Test the Inference API\n",
        "4. Integrate with your Next.js app\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 🔧 Setup Environment\n",
        "!pip install huggingface_hub transformers peft torch\n",
        "\n",
        "from huggingface_hub import HfApi, login\n",
        "import getpass\n",
        "\n",
        "# Login to Hugging Face\n",
        "print(\"Get your Hugging Face token at: https://huggingface.co/settings/tokens\")\n",
        "print(\"Make sure to create a token with 'Write' permissions!\")\n",
        "hf_token = getpass.getpass(\"Enter your Hugging Face token: \")\n",
        "login(token=hf_token)\n",
        "\n",
        "print(\"✅ Logged in to Hugging Face!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 📁 Upload Your Model to Hugging Face Hub\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from peft import PeftModel\n",
        "import torch\n",
        "import os\n",
        "\n",
        "# Configuration - UPDATE THESE!\n",
        "LOCAL_MODEL_PATH = \"/content/mistral_7b_fairness_model\"  # Path to your uploaded model folder\n",
        "HF_MODEL_NAME = \"your-username/mistral-7b-fairness\"  # Replace 'your-username' with your HF username\n",
        "\n",
        "print(f\"📁 Local model path: {LOCAL_MODEL_PATH}\")\n",
        "print(f\"🤗 Will upload to: https://huggingface.co/{HF_MODEL_NAME}\")\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "\n",
        "try:\n",
        "    # Check if local model exists\n",
        "    if not os.path.exists(LOCAL_MODEL_PATH):\n",
        "        print(f\"❌ Model not found at {LOCAL_MODEL_PATH}\")\n",
        "        print(\"Please upload your mistral_7b_fairness_model folder to this Colab session first!\")\n",
        "        print(\"Use the file browser on the left to upload the entire folder.\")\n",
        "    else:\n",
        "        print(f\"✅ Found model at {LOCAL_MODEL_PATH}\")\n",
        "        \n",
        "        # List files to verify\n",
        "        files = os.listdir(LOCAL_MODEL_PATH)\n",
        "        print(f\"📋 Model files: {files}\")\n",
        "        \n",
        "        # Load base model and adapter\n",
        "        base_model_name = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
        "        print(f\"\\n🔄 Loading base model: {base_model_name}\")\n",
        "        \n",
        "        base_model = AutoModelForCausalLM.from_pretrained(\n",
        "            base_model_name,\n",
        "            torch_dtype=torch.float16,\n",
        "            low_cpu_mem_usage=True,\n",
        "            device_map=\"auto\"\n",
        "        )\n",
        "        \n",
        "        print(f\"🔄 Loading LoRA adapters from: {LOCAL_MODEL_PATH}\")\n",
        "        model = PeftModel.from_pretrained(base_model, LOCAL_MODEL_PATH)\n",
        "        tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
        "        \n",
        "        # Merge adapter with base model for easier deployment\n",
        "        print(\"🔄 Merging LoRA adapters with base model...\")\n",
        "        merged_model = model.merge_and_unload()\n",
        "        \n",
        "        # Upload to Hugging Face Hub\n",
        "        print(f\"🚀 Uploading model to: {HF_MODEL_NAME}\")\n",
        "        merged_model.push_to_hub(\n",
        "            HF_MODEL_NAME,\n",
        "            token=hf_token,\n",
        "            private=False,  # Set to True if you want a private model\n",
        "            commit_message=\"Upload fairness-trained Mistral model\"\n",
        "        )\n",
        "        \n",
        "        tokenizer.push_to_hub(\n",
        "            HF_MODEL_NAME,\n",
        "            token=hf_token,\n",
        "            commit_message=\"Upload tokenizer\"\n",
        "        )\n",
        "        \n",
        "        print(f\"✅ Model uploaded successfully!\")\n",
        "        print(f\"🌐 Model URL: https://huggingface.co/{HF_MODEL_NAME}\")\n",
        "        print(f\"🔗 API URL: https://api-inference.huggingface.co/models/{HF_MODEL_NAME}\")\n",
        "        \n",
        "except Exception as e:\n",
        "    print(f\"❌ Error uploading model: {e}\")\n",
        "    print(\"\\n📝 Make sure to:\")\n",
        "    print(\"1. Upload your mistral_7b_fairness_model folder to this Colab session\")\n",
        "    print(\"2. Update LOCAL_MODEL_PATH to point to your uploaded folder\")\n",
        "    print(\"3. Update HF_MODEL_NAME with your Hugging Face username\")\n",
        "    print(\"4. Ensure you have a valid Hugging Face token with write permissions\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 🧪 Test Inference API\n",
        "import requests\n",
        "import json\n",
        "import time\n",
        "\n",
        "# Hugging Face Inference API endpoint\n",
        "API_URL = f\"https://api-inference.huggingface.co/models/{HF_MODEL_NAME}\"\n",
        "headers = {\"Authorization\": f\"Bearer {hf_token}\"}\n",
        "\n",
        "def query_model(prompt, max_new_tokens=50, temperature=0.7):\n",
        "    \"\"\"Query the model via Hugging Face Inference API\"\"\"\n",
        "    payload = {\n",
        "        \"inputs\": prompt,\n",
        "        \"parameters\": {\n",
        "            \"max_new_tokens\": max_new_tokens,\n",
        "            \"temperature\": temperature,\n",
        "            \"top_p\": 0.9,\n",
        "            \"do_sample\": True,\n",
        "            \"return_full_text\": False,\n",
        "            \"stop\": [\"Human:\", \"Assistant:\", \"\\n\\n\"]\n",
        "        }\n",
        "    }\n",
        "    \n",
        "    response = requests.post(API_URL, headers=headers, json=payload)\n",
        "    return response.json()\n",
        "\n",
        "# Test the model with a fairness scenario\n",
        "test_prompt = \"[INST]As a professional customer service representative, respond politely and fairly to: This service is terrible and you people don't know what you're doing![/INST]\"\n",
        "\n",
        "print(\"🧪 Testing model via Inference API...\")\n",
        "print(f\"📝 Prompt: {test_prompt}\")\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "\n",
        "# First request might take longer (model loading)\n",
        "print(\"⏳ Sending request (first request may take 30-60 seconds for model loading)...\")\n",
        "start_time = time.time()\n",
        "result = query_model(test_prompt)\n",
        "end_time = time.time()\n",
        "\n",
        "print(f\"⏱️ Response time: {end_time - start_time:.2f} seconds\")\n",
        "print(f\"📋 Raw response: {result}\")\n",
        "\n",
        "if isinstance(result, list) and len(result) > 0:\n",
        "    generated_text = result[0].get('generated_text', '')\n",
        "    print(f\"\\n✅ Generated Response: {generated_text}\")\n",
        "    print(\"\\n🎉 Inference API is working!\")\n",
        "elif 'error' in result:\n",
        "    print(f\"\\n⚠️ API Error: {result['error']}\")\n",
        "    if 'loading' in result['error'].lower():\n",
        "        print(\"💡 The model is still loading. This is normal for the first request.\")\n",
        "        print(\"   Try running this cell again in 1-2 minutes.\")\n",
        "    else:\n",
        "        print(\"💡 Check your model name and token permissions.\")\n",
        "else:\n",
        "    print(f\"\\n❓ Unexpected response format: {result}\")\n",
        "\n",
        "print(f\"\\n🔗 Your model API endpoint: {API_URL}\")\n",
        "print(f\"🌐 Model page: https://huggingface.co/{HF_MODEL_NAME}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 📋 Generate Next.js Integration Code\n",
        "integration_code = f'''// app/api/trained-model-hf/route.ts\n",
        "import {{ NextRequest }} from 'next/server';\n",
        "\n",
        "const HF_API_URL = \"https://api-inference.huggingface.co/models/{HF_MODEL_NAME}\";\n",
        "const HF_TOKEN = process.env.HUGGINGFACE_TOKEN; // Add this to your .env.local\n",
        "\n",
        "export async function POST(request: NextRequest) {{\n",
        "  try {{\n",
        "    const {{ message, persona, temperature = 0.7, max_length = 50 }} = await request.json();\n",
        "\n",
        "    if (!message) {{\n",
        "      return Response.json(\n",
        "        {{ error: 'Message is required' }},\n",
        "        {{ status: 400 }}\n",
        "      );\n",
        "    }}\n",
        "\n",
        "    if (!HF_TOKEN) {{\n",
        "      return Response.json(\n",
        "        {{ error: 'Hugging Face token not configured. Add HUGGINGFACE_TOKEN to .env.local' }},\n",
        "        {{ status: 500 }}\n",
        "      );\n",
        "    }}\n",
        "\n",
        "    // Format prompt for Mistral\n",
        "    let formatted_prompt;\n",
        "    if (persona?.trim()) {{\n",
        "      const persona_role = persona.replace(\"You are\", \"\").replace(\"Act as\", \"\").trim();\n",
        "      formatted_prompt = `[INST]As a ${{persona_role}}, respond politely and fairly to: ${{message}}[/INST]`;\n",
        "    }} else {{\n",
        "      formatted_prompt = `[INST]Respond politely and fairly to: ${{message}}[/INST]`;\n",
        "    }}\n",
        "\n",
        "    console.log('Calling Hugging Face API:', HF_API_URL);\n",
        "\n",
        "    // Call Hugging Face Inference API\n",
        "    const response = await fetch(HF_API_URL, {{\n",
        "      method: 'POST',\n",
        "      headers: {{\n",
        "        'Authorization': `Bearer ${{HF_TOKEN}}`,\n",
        "        'Content-Type': 'application/json',\n",
        "      }},\n",
        "      body: JSON.stringify({{\n",
        "        inputs: formatted_prompt,\n",
        "        parameters: {{\n",
        "          max_new_tokens: max_length,\n",
        "          temperature: temperature,\n",
        "          top_p: 0.9,\n",
        "          do_sample: true,\n",
        "          return_full_text: false,\n",
        "          stop: [\"Human:\", \"Assistant:\", \"\\\\n\\\\n\"]\n",
        "        }}\n",
        "      }}),\n",
        "      signal: AbortSignal.timeout(60000) // 60 second timeout for model loading\n",
        "    }});\n",
        "\n",
        "    const data = await response.json();\n",
        "    \n",
        "    if (data.error) {{\n",
        "      console.error('Hugging Face API error:', data.error);\n",
        "      \n",
        "      // Handle model loading\n",
        "      if (data.error.includes('loading')) {{\n",
        "        return Response.json({{\n",
        "          error: 'Model is loading, please try again in a moment',\n",
        "          details: data.error,\n",
        "          retry_after: 30\n",
        "        }}, {{ status: 503 }});\n",
        "      }}\n",
        "      \n",
        "      return Response.json(\n",
        "        {{ error: `Hugging Face API error: ${{data.error}}` }},\n",
        "        {{ status: 502 }}\n",
        "      );\n",
        "    }}\n",
        "\n",
        "    // Extract generated text\n",
        "    let generated_text = \"Sorry, I couldn't generate a response.\";\n",
        "    if (Array.isArray(data) && data.length > 0) {{\n",
        "      generated_text = data[0]?.generated_text || generated_text;\n",
        "    }}\n",
        "    \n",
        "    return Response.json({{\n",
        "      response: generated_text.trim(),\n",
        "      model: \"{HF_MODEL_NAME}\",\n",
        "      fairness_enabled: true,\n",
        "      processing_time_ms: 0,\n",
        "      gpu_used: true,\n",
        "      cloud_inference: true,\n",
        "      provider: \"huggingface\"\n",
        "    }});\n",
        "\n",
        "  }} catch (error: any) {{\n",
        "    console.error('Error calling Hugging Face API:', error);\n",
        "    \n",
        "    if (error.name === 'AbortError') {{\n",
        "      return Response.json(\n",
        "        {{ error: 'Request timed out - model may be loading' }},\n",
        "        {{ status: 504 }}\n",
        "      );\n",
        "    }}\n",
        "\n",
        "    return Response.json(\n",
        "      {{ error: 'Failed to get response from model: ' + error.message }},\n",
        "      {{ status: 500 }}\n",
        "    );\n",
        "  }}\n",
        "}}\n",
        "\n",
        "export async function GET() {{\n",
        "  return Response.json({{\n",
        "    message: \"Trained Model Hugging Face API\",\n",
        "    model: \"{HF_MODEL_NAME}\",\n",
        "    provider: \"huggingface\",\n",
        "    status: \"ready\",\n",
        "    api_url: HF_API_URL\n",
        "  }});\n",
        "}}'''\n",
        "\n",
        "print(\"📋 Next.js Integration Code:\")\n",
        "print(\"=\"*50)\n",
        "print(integration_code)\n",
        "\n",
        "print(f\"\\n🔑 Environment Variable:\")\n",
        "print(f\"Add this to your .env.local file:\")\n",
        "print(f\"HUGGINGFACE_TOKEN={hf_token}\")\n",
        "\n",
        "print(f\"\\n🌐 Your model details:\")\n",
        "print(f\"• Model URL: https://huggingface.co/{HF_MODEL_NAME}\")\n",
        "print(f\"• API endpoint: https://api-inference.huggingface.co/models/{HF_MODEL_NAME}\")\n",
        "print(f\"• Next.js route: /api/trained-model-hf\")\n",
        "\n",
        "print(f\"\\n📝 Next steps:\")\n",
        "print(f\"1. Copy the code above and create app/api/trained-model-hf/route.ts\")\n",
        "print(f\"2. Add HUGGINGFACE_TOKEN to your .env.local\")\n",
        "print(f\"3. Update your chat component to use /api/trained-model-hf\")\n",
        "print(f\"4. Test your fairness model!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## 🎯 Summary\n",
        "\n",
        "Your trained Mistral fairness model is now available on Hugging Face Hub! Here's what you can do:\n",
        "\n",
        "### ✅ Advantages of Hugging Face Inference API:\n",
        "- **No server management** - Hugging Face handles everything\n",
        "- **Automatic scaling** - Handles traffic spikes automatically\n",
        "- **Built-in optimization** - Faster inference with caching\n",
        "- **Free tier** - Good for development and testing\n",
        "- **Easy integration** - Simple REST API calls\n",
        "- **Always available** - No session timeouts like Colab\n",
        "\n",
        "### 🔄 Next Steps:\n",
        "1. Copy the generated Next.js code above\n",
        "2. Create `app/api/trained-model-hf/route.ts` with the code\n",
        "3. Add your Hugging Face token to `.env.local`\n",
        "4. Update your chat component to use `/api/trained-model-hf`\n",
        "5. Test your fairness-trained model!\n",
        "\n",
        "### 💡 Tips:\n",
        "- The model might take a few minutes to \"warm up\" on first use\n",
        "- Free tier has rate limits - consider upgrading for production\n",
        "- You can make your model private if needed\n",
        "- Monitor usage on your Hugging Face dashboard\n",
        "\n",
        "### 🆚 Comparison with Google Colab:\n",
        "- **Colab**: Free GPU, but requires keeping session active\n",
        "- **Hugging Face**: Always available, but free tier has limits\n",
        "- **Production**: Consider paid tiers for both options\n",
        "\n",
        "### 🔧 Troubleshooting:\n",
        "- **Model loading errors**: Wait 1-2 minutes and try again\n",
        "- **Rate limits**: Upgrade to paid tier or wait for reset\n",
        "- **Token errors**: Make sure token has write permissions\n",
        "- **API errors**: Check the Hugging Face status page\n",
        "\n",
        "Your fairness-trained model is now serverless and ready to use! 🚀\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
