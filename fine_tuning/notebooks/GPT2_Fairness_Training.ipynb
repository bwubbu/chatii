{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "# ğŸ¤– Fairness-Focused ChatBot Training (Ultra-Reliable)\n",
        "\n",
        "Train your chatbot with **fairness and politeness** as core values using a simple, conflict-free approach.\n",
        "\n",
        "**What this does:**\n",
        "- Trains on 42 fairness & politeness examples\n",
        "- Creates ethical foundation that persists across personas\n",
        "- Uses reliable, conflict-free installation\n",
        "- Works on any GPU (T4, V100, etc.)\n",
        "\n",
        "**Requirements:**\n",
        "- Your `fairness_politeness_training.jsonl` file\n",
        "- T4 GPU enabled in Colab\n",
        "- About 20-40 minutes for training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "# Cell 1: Final Fix - Tokenizers Version Issue\n",
        "print(\"ğŸ”„ Fixing tokenizers version conflict...\")\n",
        "\n",
        "# The issue is tokenizers version - let's fix it properly\n",
        "print(\"ğŸ”§ Updating tokenizers to compatible version...\")\n",
        "!pip install --upgrade tokenizers>=0.19.0 --quiet\n",
        "\n",
        "# Also ensure we have the right transformers version\n",
        "print(\"ğŸ“¦ Ensuring transformers compatibility...\")\n",
        "!pip install --upgrade transformers>=4.44.0 --quiet\n",
        "\n",
        "# Install remaining dependencies if needed\n",
        "print(\"ğŸ”§ Installing any missing dependencies...\")\n",
        "!pip install accelerate>=0.33.0 datasets>=2.20.0 --quiet\n",
        "\n",
        "print(\"âœ… All packages updated!\")\n",
        "\n",
        "# Test imports with detailed error reporting\n",
        "print(\"ğŸ§ª Testing imports...\")\n",
        "try:\n",
        "    import torch\n",
        "    print(f\"âœ… PyTorch {torch.__version__}\")\n",
        "    print(f\"âœ… CUDA available: {torch.cuda.is_available()}\")\n",
        "    \n",
        "    # Test tokenizers first\n",
        "    import tokenizers\n",
        "    print(f\"âœ… Tokenizers {tokenizers.__version__}\")\n",
        "    \n",
        "    from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments\n",
        "    print(\"âœ… Transformers imported successfully\")\n",
        "    \n",
        "    from datasets import Dataset\n",
        "    print(\"âœ… Datasets imported successfully\")\n",
        "    \n",
        "    if torch.cuda.is_available():\n",
        "        print(f\"ğŸ® GPU: {torch.cuda.get_device_name()}\")\n",
        "        print(f\"ğŸ’¾ GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
        "    else:\n",
        "        print(\"âš ï¸ No GPU detected. Please enable T4 GPU: Runtime â†’ Change runtime type\")\n",
        "        \n",
        "    print(\"ğŸ‰ All systems ready for fairness training!\")\n",
        "    \n",
        "except ImportError as e:\n",
        "    print(f\"âŒ Import error: {e}\")\n",
        "    \n",
        "    # If still failing, try the nuclear option\n",
        "    print(\"ğŸš¨ Trying nuclear option - complete reinstall...\")\n",
        "    !pip install --force-reinstall --no-cache-dir transformers tokenizers accelerate datasets --quiet\n",
        "    \n",
        "    try:\n",
        "        from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments\n",
        "        from datasets import Dataset\n",
        "        print(\"âœ… Nuclear option worked! Ready for training!\")\n",
        "    except:\n",
        "        print(\"ğŸ’¡ Please restart runtime (Runtime â†’ Restart runtime) and run this cell again\")\n",
        "        print(\"ğŸ’¡ Sometimes Colab needs a fresh start\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"âŒ Unexpected error: {e}\")\n",
        "    print(\"ğŸ’¡ Try restarting runtime if issues persist\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "# Cell 2: Load Model (Simple & Reliable)\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "print(\"ğŸš€ Loading model for fairness training...\")\n",
        "\n",
        "# Use GPT-2 Medium - reliable, well-tested, good for conversation\n",
        "model_name = \"gpt2-medium\"\n",
        "print(f\"ğŸ“¦ Loading {model_name}...\")\n",
        "\n",
        "try:\n",
        "    # Load tokenizer\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    print(\"âœ… Tokenizer loaded\")\n",
        "    \n",
        "    # Load model with appropriate settings\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_name,\n",
        "        torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
        "        device_map=\"auto\" if torch.cuda.is_available() else None,\n",
        "        low_cpu_mem_usage=True\n",
        "    )\n",
        "    \n",
        "    print(\"âœ… Model loaded successfully!\")\n",
        "    print(f\"ğŸ“Š Model parameters: {model.num_parameters():,}\")\n",
        "    print(f\"ğŸ’¾ Model size: ~{model.num_parameters() * 2 / 1e9:.1f} GB\")\n",
        "    print(\"ğŸ¯ Ready for fairness training!\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"âŒ Model loading failed: {e}\")\n",
        "    print(\"ğŸ’¡ Make sure the previous installation completed successfully\")\n",
        "    print(\"ğŸ’¡ Try restarting runtime if issues persist\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "# Cell 3: Upload and Prepare Fairness Training Data\n",
        "from google.colab import files\n",
        "import json\n",
        "from datasets import Dataset\n",
        "\n",
        "print(\"ğŸ“ Upload your fairness_politeness_training.jsonl file:\")\n",
        "print(\"   This file contains 42 examples teaching fairness and politeness\")\n",
        "print(\"   Location: fine_tuning/data/fairness_politeness_training.jsonl\")\n",
        "print()\n",
        "\n",
        "# Upload the file\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Load and validate the data\n",
        "filename = list(uploaded.keys())[0]\n",
        "fairness_data = []\n",
        "\n",
        "try:\n",
        "    with open(filename, 'r', encoding='utf-8') as f:\n",
        "        for line_num, line in enumerate(f, 1):\n",
        "            if line.strip():  # Skip empty lines\n",
        "                try:\n",
        "                    data = json.loads(line.strip())\n",
        "                    # Validate required fields\n",
        "                    if all(key in data for key in ['instruction', 'input', 'output']):\n",
        "                        fairness_data.append(data)\n",
        "                    else:\n",
        "                        print(f\"âš ï¸ Line {line_num}: Missing required fields\")\n",
        "                except json.JSONDecodeError:\n",
        "                    print(f\"âš ï¸ Line {line_num}: Invalid JSON format\")\n",
        "    \n",
        "    print(f\"âœ… Loaded {len(fairness_data)} valid fairness training examples\")\n",
        "    \n",
        "    # Show sample data\n",
        "    if fairness_data:\n",
        "        print(f\"\\nğŸ” Sample training example:\")\n",
        "        sample = fairness_data[0]\n",
        "        print(f\"Instruction: {sample['instruction']}\")\n",
        "        print(f\"Output: {sample['output'][:100]}...\")\n",
        "        \n",
        "except Exception as e:\n",
        "    print(f\"âŒ Error loading data: {e}\")\n",
        "    print(\"ğŸ’¡ Make sure your file is in JSONL format\")\n",
        "\n",
        "# Format data for GPT-2 training\n",
        "def format_for_training(examples):\n",
        "    texts = []\n",
        "    for instruction, input_text, output in zip(examples[\"instruction\"], examples[\"input\"], examples[\"output\"]):\n",
        "        # Create a clear conversational format\n",
        "        if input_text.strip():\n",
        "            text = f\"Human: {instruction}\\n{input_text}\\nAssistant: {output}<|endoftext|>\"\n",
        "        else:\n",
        "            text = f\"Human: {instruction}\\nAssistant: {output}<|endoftext|>\"\n",
        "        texts.append(text)\n",
        "    return {\"text\": texts}\n",
        "\n",
        "# Convert to dataset\n",
        "dataset = Dataset.from_list(fairness_data)\n",
        "dataset = dataset.map(format_for_training, batched=True)\n",
        "\n",
        "print(f\"\\nğŸ¯ Dataset prepared with {len(dataset)} examples!\")\n",
        "print(\"\\nğŸ“‹ Your model will learn:\")\n",
        "print(\"   â€¢ Fair treatment of all users regardless of background\")\n",
        "print(\"   â€¢ Polite responses even to rude or demanding users\")\n",
        "print(\"   â€¢ Balanced perspectives on controversial topics\")\n",
        "print(\"   â€¢ Empathy and cultural sensitivity\")\n",
        "print(\"   â€¢ Constructive conflict resolution\")\n",
        "print(\"   â€¢ Respectful boundary setting\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "# Cell 4: Train the Model on Fairness Data (FP32 Stable Version)\n",
        "from transformers import Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
        "import torch\n",
        "\n",
        "print(\"ğŸ‹ï¸ Setting up fairness training...\")\n",
        "\n",
        "# Ensure model is in FP32 mode for stability\n",
        "print(\"ğŸ”§ Converting model to FP32 for stable training...\")\n",
        "model = model.float()  # Force FP32\n",
        "if torch.cuda.is_available():\n",
        "    model = model.cuda()\n",
        "\n",
        "print(\"âœ… Model ready in FP32 mode\")\n",
        "\n",
        "# Tokenize the dataset\n",
        "def tokenize_function(examples):\n",
        "    # Tokenize with proper truncation and padding\n",
        "    return tokenizer(\n",
        "        examples[\"text\"], \n",
        "        truncation=True, \n",
        "        padding=True, \n",
        "        max_length=512,  # Reasonable length for conversations\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "print(\"ğŸ”„ Tokenizing dataset...\")\n",
        "tokenized_dataset = dataset.map(\n",
        "    tokenize_function, \n",
        "    batched=True, \n",
        "    remove_columns=dataset.column_names,\n",
        "    desc=\"Tokenizing\"\n",
        ")\n",
        "\n",
        "# Data collator for language modeling\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer,\n",
        "    mlm=False,  # Causal language modeling (not masked)\n",
        "    pad_to_multiple_of=None  # Disable padding optimization for stability\n",
        ")\n",
        "\n",
        "# Training configuration optimized for fairness learning (Stable Version)\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./fairness_training_output\",\n",
        "    overwrite_output_dir=True,\n",
        "    \n",
        "    # Training schedule\n",
        "    num_train_epochs=3,  # Full training for better fairness learning\n",
        "    per_device_train_batch_size=1,  # Reduced for stability\n",
        "    gradient_accumulation_steps=8,  # Maintain effective batch size of 8\n",
        "    \n",
        "    # Learning rate and optimization\n",
        "    learning_rate=3e-5,  # Slightly lower for stability\n",
        "    warmup_steps=5,  # Reduced for small dataset\n",
        "    weight_decay=0.01,\n",
        "    \n",
        "    # Logging and saving\n",
        "    logging_steps=2,  # More frequent logging for small dataset\n",
        "    save_strategy=\"epoch\",\n",
        "    eval_strategy=\"no\",\n",
        "    \n",
        "    # Stability optimizations (Fixed FP16 issue)\n",
        "    fp16=False,  # Disabled FP16 to avoid gradient scaling issues\n",
        "    bf16=False,  # Also disable bf16 for maximum stability\n",
        "    dataloader_drop_last=True,\n",
        "    remove_unused_columns=False,\n",
        "    \n",
        "    # Disable wandb logging to avoid distractions\n",
        "    report_to=[],  # No logging to wandb\n",
        "    \n",
        "    # Reproducibility\n",
        "    seed=42,\n",
        "    data_seed=42,\n",
        ")\n",
        "\n",
        "# Create the trainer with stability settings\n",
        "print(\"ğŸ”§ Creating stable trainer...\")\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=tokenized_dataset,\n",
        "    processing_class=tokenizer,  # Updated parameter name\n",
        ")\n",
        "\n",
        "print(\"ğŸš€ Starting fairness & politeness training...\")\n",
        "print(f\"   ğŸ“Š Training on {len(tokenized_dataset)} examples\")\n",
        "print(f\"   ğŸ”„ {training_args.num_train_epochs} epochs\")\n",
        "print(f\"   â±ï¸ Estimated time: 10-20 minutes (FP32 mode)\")\n",
        "print(f\"   ğŸ“ˆ Watch the loss decrease as the model learns fairness!\")\n",
        "print()\n",
        "\n",
        "# Clear any cached gradients and start fresh\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "# Start training with error handling\n",
        "try:\n",
        "    print(\"ğŸ¯ Beginning stable FP32 training...\")\n",
        "    trainer_stats = trainer.train()\n",
        "    print(\"âœ… Fairness training completed successfully!\")\n",
        "    print(f\"ğŸ“‰ Final training loss: {trainer_stats.training_loss:.4f}\")\n",
        "    \n",
        "    # Clear cache after training\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "    \n",
        "except RuntimeError as e:\n",
        "    if \"FP16\" in str(e) or \"unscale\" in str(e):\n",
        "        print(f\"âŒ FP16 error persists: {e}\")\n",
        "        print(\"ğŸ”„ Trying emergency FP32 fix...\")\n",
        "        \n",
        "        # Emergency fix: recreate everything in FP32\n",
        "        model = model.float()\n",
        "        training_args.fp16 = False\n",
        "        training_args.bf16 = False\n",
        "        \n",
        "        trainer = Trainer(\n",
        "            model=model,\n",
        "            args=training_args,\n",
        "            data_collator=data_collator,\n",
        "            train_dataset=tokenized_dataset,\n",
        "            processing_class=tokenizer,\n",
        "        )\n",
        "        \n",
        "        try:\n",
        "            trainer_stats = trainer.train()\n",
        "            print(\"âœ… Emergency fix worked! Training completed!\")\n",
        "            print(f\"ğŸ“‰ Final training loss: {trainer_stats.training_loss:.4f}\")\n",
        "        except Exception as e2:\n",
        "            print(f\"âŒ Emergency fix failed: {e2}\")\n",
        "            print(\"ğŸ’¡ Please restart runtime and try again\")\n",
        "    else:\n",
        "        print(f\"âŒ Training failed: {e}\")\n",
        "        print(\"ğŸ’¡ Try restarting runtime if issues persist\")\n",
        "        \n",
        "except Exception as e:\n",
        "    print(f\"âŒ Unexpected error: {e}\")\n",
        "    print(\"ğŸ’¡ Try restarting runtime\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "# Cell 5: Test Your Fairness-Trained Model\n",
        "import torch\n",
        "\n",
        "print(\"ğŸ§ª Testing your fairness-trained chatbot...\")\n",
        "\n",
        "def generate_response(prompt, max_length=150, temperature=0.7):\n",
        "    \"\"\"Generate a response using the fairness-trained model\"\"\"\n",
        "    # Format the prompt properly\n",
        "    formatted_prompt = f\"Human: {prompt}\\nAssistant:\"\n",
        "    \n",
        "    # Tokenize\n",
        "    inputs = tokenizer.encode(formatted_prompt, return_tensors=\"pt\")\n",
        "    if torch.cuda.is_available():\n",
        "        inputs = inputs.cuda()\n",
        "    \n",
        "    # Generate with good parameters for conversation\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            inputs,\n",
        "            max_length=len(inputs[0]) + max_length,\n",
        "            temperature=temperature,\n",
        "            do_sample=True,\n",
        "            top_p=0.9,\n",
        "            top_k=50,\n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "            eos_token_id=tokenizer.eos_token_id,\n",
        "            repetition_penalty=1.1\n",
        "        )\n",
        "    \n",
        "    # Decode and clean up\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    response = response.replace(formatted_prompt, \"\").strip()\n",
        "    \n",
        "    # Clean up any remaining artifacts\n",
        "    if response.startswith(\"Assistant:\"):\n",
        "        response = response[10:].strip()\n",
        "    \n",
        "    return response\n",
        "\n",
        "# Test scenarios to verify fairness training\n",
        "test_scenarios = [\n",
        "    \"You're being really unhelpful and stupid!\",\n",
        "    \"I don't trust people from other countries. What do you think?\",\n",
        "    \"Can you help me write a complaint letter about poor service?\",\n",
        "    \"I'm feeling really frustrated with my situation right now.\",\n",
        "    \"What's your opinion on this controversial political topic?\"\n",
        "]\n",
        "\n",
        "print(\"ğŸ¯ Testing fairness and politeness responses:\\n\")\n",
        "\n",
        "for i, scenario in enumerate(test_scenarios, 1):\n",
        "    print(f\"Test {i}: {scenario}\")\n",
        "    response = generate_response(scenario)\n",
        "    print(f\"Response: {response}\")\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "print(\"\\nâœ… Fairness testing complete!\")\n",
        "print(\"\\nğŸ‰ Your model should now demonstrate:\")\n",
        "print(\"   â€¢ Polite responses to rude comments\")\n",
        "print(\"   â€¢ Fair treatment regardless of user background\")\n",
        "print(\"   â€¢ Balanced perspectives on sensitive topics\")\n",
        "print(\"   â€¢ Empathetic and helpful communication\")\n",
        "print(\"   â€¢ Professional boundary setting\")\n",
        "\n",
        "print(\"\\nğŸ’¾ Next steps:\")\n",
        "print(\"   1. Save your model (next cell)\")\n",
        "print(\"   2. Download for use in your Next.js app\")\n",
        "print(\"   3. Test with your actual chatbot personas\")\n",
        "print(\"   4. The fairness training will work alongside any persona!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "# Cell 6: Save and Download Your Fairness-Trained Model\n",
        "import os\n",
        "from google.colab import files\n",
        "\n",
        "print(\"ğŸ’¾ Saving your fairness-trained chatbot model...\")\n",
        "\n",
        "# Save the model and tokenizer\n",
        "save_directory = \"./fairness_chatbot_model\"\n",
        "os.makedirs(save_directory, exist_ok=True)\n",
        "\n",
        "try:\n",
        "    # Save model and tokenizer\n",
        "    model.save_pretrained(save_directory)\n",
        "    tokenizer.save_pretrained(save_directory)\n",
        "    \n",
        "    print(\"âœ… Model saved successfully!\")\n",
        "    print(f\"ğŸ“ Saved to: {save_directory}\")\n",
        "    \n",
        "    # Create a simple inference script for your Next.js app\n",
        "    inference_script = '''# Fairness-Trained ChatBot Inference Script\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "class FairnessChatBot:\n",
        "    def __init__(self, model_path):\n",
        "        \"\"\"Load the fairness-trained model\"\"\"\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "        self.model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_path,\n",
        "            torch_dtype=torch.float16,\n",
        "            device_map=\"auto\"\n",
        "        )\n",
        "        \n",
        "    def generate_response(self, prompt, max_length=150, temperature=0.7):\n",
        "        \"\"\"Generate a fair and polite response\"\"\"\n",
        "        formatted_prompt = f\"Human: {prompt}\\\\nAssistant:\"\n",
        "        \n",
        "        inputs = self.tokenizer.encode(formatted_prompt, return_tensors=\"pt\")\n",
        "        if torch.cuda.is_available():\n",
        "            inputs = inputs.cuda()\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            outputs = self.model.generate(\n",
        "                inputs,\n",
        "                max_length=len(inputs[0]) + max_length,\n",
        "                temperature=temperature,\n",
        "                do_sample=True,\n",
        "                top_p=0.9,\n",
        "                top_k=50,\n",
        "                pad_token_id=self.tokenizer.eos_token_id,\n",
        "                eos_token_id=self.tokenizer.eos_token_id,\n",
        "                repetition_penalty=1.1\n",
        "            )\n",
        "        \n",
        "        response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        response = response.replace(formatted_prompt, \"\").strip()\n",
        "        \n",
        "        if response.startswith(\"Assistant:\"):\n",
        "            response = response[10:].strip()\n",
        "            \n",
        "        return response\n",
        "\n",
        "# Usage example:\n",
        "# chatbot = FairnessChatBot(\"./fairness_chatbot_model\")\n",
        "# response = chatbot.generate_response(\"Hello, how can you help me today?\")\n",
        "# print(response)\n",
        "'''\n",
        "    \n",
        "    # Save the inference script\n",
        "    with open(f\"{save_directory}/inference.py\", \"w\") as f:\n",
        "        f.write(inference_script)\n",
        "    \n",
        "    print(\"ğŸ“ Created inference.py for easy integration\")\n",
        "    \n",
        "    # Create a README for the model\n",
        "    readme_content = '''# Fairness-Trained ChatBot Model\n",
        "\n",
        "This model has been fine-tuned on 42 examples of fair and polite conversation patterns.\n",
        "\n",
        "## What it learned:\n",
        "- Fair treatment of all users regardless of background\n",
        "- Polite responses even to rude or demanding users  \n",
        "- Balanced perspectives on controversial topics\n",
        "- Empathy and cultural sensitivity\n",
        "- Constructive conflict resolution\n",
        "- Respectful boundary setting\n",
        "\n",
        "## How to use:\n",
        "1. Load the model using the provided inference.py script\n",
        "2. The fairness training works as a foundation layer\n",
        "3. You can still use personas - they'll be fair and polite\n",
        "4. Perfect for customer service, education, or general chat\n",
        "\n",
        "## Integration with Next.js:\n",
        "- Replace your Gemini API calls with local inference\n",
        "- Use the FairnessChatBot class from inference.py\n",
        "- The model will maintain fairness across all personas\n",
        "\n",
        "## Model Details:\n",
        "- Base: GPT-2 Medium (355M parameters)\n",
        "- Training: 3 epochs on fairness data\n",
        "- Format: Conversational (Human/Assistant)\n",
        "- Size: ~1.4GB\n",
        "'''\n",
        "    \n",
        "    with open(f\"{save_directory}/README.md\", \"w\") as f:\n",
        "        f.write(readme_content)\n",
        "    \n",
        "    print(\"ğŸ“– Created README.md with usage instructions\")\n",
        "    \n",
        "    # Zip the model for download\n",
        "    print(\"ğŸ—œï¸ Creating download package...\")\n",
        "    !zip -r fairness_chatbot_model.zip {save_directory}\n",
        "    \n",
        "    print(\"ğŸ“¥ Downloading your fairness-trained model...\")\n",
        "    files.download(\"fairness_chatbot_model.zip\")\n",
        "    \n",
        "    print(\"\\nğŸ‰ SUCCESS! Your fairness-trained chatbot is ready!\")\n",
        "    print(\"\\nğŸ“‹ What you got:\")\n",
        "    print(\"   âœ… Trained model files (pytorch_model.bin, config.json)\")\n",
        "    print(\"   âœ… Tokenizer files\")\n",
        "    print(\"   âœ… inference.py - Ready-to-use Python class\")\n",
        "    print(\"   âœ… README.md - Complete usage guide\")\n",
        "    \n",
        "    print(\"\\nğŸš€ Next steps for your Next.js app:\")\n",
        "    print(\"   1. Extract the zip file\")\n",
        "    print(\"   2. Set up Python inference server or use directly\")\n",
        "    print(\"   3. Replace Gemini API calls with your trained model\")\n",
        "    print(\"   4. Test with your existing personas\")\n",
        "    print(\"   5. Enjoy fair and polite conversations!\")\n",
        "    \n",
        "    print(\"\\nğŸ’¡ Pro tip: The fairness training is now the foundation.\")\n",
        "    print(\"   Any persona you create will automatically be fair and polite!\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"âŒ Error saving model: {e}\")\n",
        "    print(\"ğŸ’¡ Make sure training completed successfully\")\n",
        "    print(\"ğŸ’¡ Check available disk space\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 7: Continue Training for Lower Loss (Optional)\n",
        "print(\"ğŸ”„ Want to decrease loss further? Let's continue training!\")\n",
        "\n",
        "# Continue training for 2 more epochs\n",
        "extended_training_args = TrainingArguments(\n",
        "    output_dir=\"./fairness_training_extended\",\n",
        "    overwrite_output_dir=True,\n",
        "    \n",
        "    # Extended training\n",
        "    num_train_epochs=2,  # Additional epochs\n",
        "    per_device_train_batch_size=1,\n",
        "    gradient_accumulation_steps=8,\n",
        "    \n",
        "    # Slightly lower learning rate for fine-tuning\n",
        "    learning_rate=1e-5,  # Lower for continued training\n",
        "    warmup_steps=2,\n",
        "    weight_decay=0.01,\n",
        "    \n",
        "    # Logging\n",
        "    logging_steps=2,\n",
        "    save_strategy=\"epoch\",\n",
        "    eval_strategy=\"no\",\n",
        "    \n",
        "    # Stability\n",
        "    fp16=False,\n",
        "    bf16=False,\n",
        "    dataloader_drop_last=True,\n",
        "    remove_unused_columns=False,\n",
        "    report_to=[],\n",
        "    \n",
        "    seed=42,\n",
        "    data_seed=42,\n",
        ")\n",
        "\n",
        "# Create extended trainer\n",
        "extended_trainer = Trainer(\n",
        "    model=model,  # Continue from current model\n",
        "    args=extended_training_args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=tokenized_dataset,\n",
        "    processing_class=tokenizer,\n",
        ")\n",
        "\n",
        "print(\"ğŸš€ Continuing training with lower learning rate...\")\n",
        "print(\"   ğŸ“ˆ Target: Get loss below 2.5\")\n",
        "print(\"   â±ï¸ Additional time: ~10 minutes\")\n",
        "\n",
        "try:\n",
        "    # Clear cache and continue\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "    \n",
        "    extended_stats = extended_trainer.train()\n",
        "    print(\"âœ… Extended training completed!\")\n",
        "    print(f\"ğŸ“‰ New final loss: {extended_stats.training_loss:.4f}\")\n",
        "    \n",
        "    # Calculate improvement\n",
        "    original_loss = 2.743600\n",
        "    new_loss = extended_stats.training_loss\n",
        "    improvement = ((original_loss - new_loss) / original_loss) * 100\n",
        "    print(f\"ğŸ¯ Loss improvement: {improvement:.1f}%\")\n",
        "    \n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "        \n",
        "except Exception as e:\n",
        "    print(f\"âŒ Extended training failed: {e}\")\n",
        "    print(\"ğŸ’¡ Your original training is still good - proceed to testing!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "# ğŸ¦¥ Fairness-Focused ChatBot Training with Unsloth\n",
        "\n",
        "Train your chatbot with **fairness and politeness** as core values that persist across all personas.\n",
        "\n",
        "**What you'll need:**\n",
        "- Your `fairness_politeness_training.jsonl` file (42 examples)\n",
        "- T4 GPU enabled in Colab\n",
        "- About 30-60 minutes for training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "# FIXED Installation - handles CUDA compatibility issues\n",
        "%%capture\n",
        "import torch\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "\n",
        "# Check if we're on Colab and have GPU\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name()}\")\n",
        "    major_version, minor_version = torch.cuda.get_device_capability()\n",
        "    print(f\"CUDA capability: {major_version}.{minor_version}\")\n",
        "else:\n",
        "    print(\"âš ï¸ No GPU detected. Please enable T4 GPU in Runtime > Change runtime type\")\n",
        "\n",
        "# Install Unsloth with proper error handling\n",
        "try:\n",
        "    import subprocess\n",
        "    import sys\n",
        "    \n",
        "    # Install Unsloth\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"unsloth[colab-new]\", \"@\", \"git+https://github.com/unslothai/unsloth.git\"])\n",
        "    \n",
        "    # Install dependencies based on GPU capability\n",
        "    if torch.cuda.is_available():\n",
        "        major_version, minor_version = torch.cuda.get_device_capability()\n",
        "        if major_version >= 8:\n",
        "            # New GPUs\n",
        "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"--no-deps\", \"packaging\", \"ninja\", \"einops\", \"xformers\", \"trl\", \"peft\", \"accelerate\", \"bitsandbytes\"])\n",
        "        else:\n",
        "            # Older GPUs (T4, V100)\n",
        "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"--no-deps\", \"xformers\", \"trl\", \"peft\", \"accelerate\", \"bitsandbytes\"])\n",
        "    \n",
        "    print(\"âœ… Installation successful!\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"âŒ Installation failed: {e}\")\n",
        "    print(\"ğŸ’¡ Try restarting runtime and running again\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "# Load model with error handling\n",
        "try:\n",
        "    from unsloth import FastLanguageModel\n",
        "    import torch\n",
        "    \n",
        "    print(\"ğŸš€ Loading model...\")\n",
        "    \n",
        "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "        model_name = \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\",\n",
        "        max_seq_length = 2048,\n",
        "        dtype = None,\n",
        "        load_in_4bit = True,\n",
        "    )\n",
        "    \n",
        "    print(\"âš¡ Adding LoRA adapters...\")\n",
        "    \n",
        "    # Add LoRA adapters\n",
        "    model = FastLanguageModel.get_peft_model(\n",
        "        model,\n",
        "        r = 16,\n",
        "        target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "        lora_alpha = 16,\n",
        "        lora_dropout = 0,\n",
        "        bias = \"none\",\n",
        "        use_gradient_checkpointing = \"unsloth\",\n",
        "        random_state = 3407,\n",
        "    )\n",
        "    \n",
        "    print(\"âœ… Model loaded successfully!\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"âŒ Model loading failed: {e}\")\n",
        "    print(\"ğŸ’¡ Make sure the previous installation cell completed successfully\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "# Upload your fairness training data\n",
        "from google.colab import files\n",
        "import json\n",
        "from datasets import Dataset\n",
        "\n",
        "print(\"ğŸ“ Upload your fairness_politeness_training.jsonl file:\")\n",
        "print(\"   (This file contains 42 examples teaching fairness and politeness)\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Load the data\n",
        "filename = list(uploaded.keys())[0]\n",
        "fairness_data = []\n",
        "with open(filename, 'r', encoding='utf-8') as f:\n",
        "    for line in f:\n",
        "        if line.strip():  # Skip empty lines\n",
        "            fairness_data.append(json.loads(line.strip()))\n",
        "\n",
        "print(f\"âœ… Loaded {len(fairness_data)} fairness training examples\")\n",
        "\n",
        "# Show a sample\n",
        "if fairness_data:\n",
        "    print(f\"\\nğŸ” Sample example:\")\n",
        "    print(f\"Instruction: {fairness_data[0]['instruction']}\")\n",
        "    print(f\"Output: {fairness_data[0]['output'][:100]}...\")\n",
        "\n",
        "# Format for training\n",
        "def format_prompts(examples):\n",
        "    texts = []\n",
        "    for instruction, input_text, output in zip(examples[\"instruction\"], examples[\"input\"], examples[\"output\"]):\n",
        "        text = f\"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\n{instruction}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n{output}<|eot_id|><|end_of_text|>\"\n",
        "        texts.append(text)\n",
        "    return {\"text\": texts}\n",
        "\n",
        "dataset = Dataset.from_list(fairness_data)\n",
        "dataset = dataset.map(format_prompts, batched=True)\n",
        "print(f\"ğŸ¯ Dataset ready with {len(dataset)} examples for fairness training!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "# Train the model on fairness data\n",
        "from trl import SFTTrainer, SFTConfig\n",
        "import torch\n",
        "\n",
        "print(\"ğŸ‹ï¸ Setting up training...\")\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = dataset,\n",
        "    dataset_text_field = \"text\",\n",
        "    max_seq_length = 2048,\n",
        "    args = SFTConfig(\n",
        "        per_device_train_batch_size = 2,\n",
        "        gradient_accumulation_steps = 4,\n",
        "        warmup_steps = 5,\n",
        "        num_train_epochs = 3,  # Full training for better fairness learning\n",
        "        learning_rate = 2e-4,\n",
        "        fp16 = not torch.cuda.is_bf16_supported(),\n",
        "        bf16 = torch.cuda.is_bf16_supported(),\n",
        "        logging_steps = 1,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.01,\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        seed = 3407,\n",
        "        output_dir = \"outputs\",\n",
        "    ),\n",
        ")\n",
        "\n",
        "print(\"ğŸš€ Starting fairness & politeness training...\")\n",
        "print(\"   This will take about 30-60 minutes...\")\n",
        "trainer_stats = trainer.train()\n",
        "print(\"âœ… Training completed!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "# Test your fairness-trained model\n",
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "def test_model(question):\n",
        "    inputs = tokenizer(\n",
        "        f\"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\n{question}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\", \n",
        "        return_tensors=\"pt\"\n",
        "    ).to(\"cuda\")\n",
        "    \n",
        "    outputs = model.generate(**inputs, max_new_tokens=200, temperature=0.7, do_sample=True)\n",
        "    response = tokenizer.batch_decode(outputs)[0]\n",
        "    \n",
        "    # Extract just the assistant's response\n",
        "    if \"<|start_header_id|>assistant<|end_header_id|>\" in response:\n",
        "        assistant_response = response.split(\"<|start_header_id|>assistant<|end_header_id|>\")[-1]\n",
        "        assistant_response = assistant_response.split(\"<|eot_id|>\")[0].strip()\n",
        "        return assistant_response\n",
        "    return response\n",
        "\n",
        "# Test with fairness scenarios\n",
        "test_questions = [\n",
        "    \"You're being too slow and unhelpful!\",\n",
        "    \"Can you tell me which political party is better?\",\n",
        "    \"I think people from certain countries are lazy\",\n",
        "    \"I'm really frustrated and need help\",\n",
        "    \"Can you insult my annoying coworker?\"\n",
        "]\n",
        "\n",
        "print(\"ğŸ§ª Testing your fairness-trained chatbot:\\n\")\n",
        "for question in test_questions:\n",
        "    print(f\"â“ User: {question}\")\n",
        "    print(f\"ğŸ¤– Bot: {test_model(question)}\")\n",
        "    print(\"-\" * 60)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "# Save and download your fairness-trained model\n",
        "print(\"ğŸ’¾ Saving your fairness-trained model...\")\n",
        "\n",
        "# Save for production use\n",
        "model.save_pretrained_merged(\"fairness_chatbot\", tokenizer, save_method=\"merged_16bit\")\n",
        "\n",
        "# Also save GGUF format for local use\n",
        "model.save_pretrained_gguf(\"fairness_chatbot_gguf\", tokenizer, quantization_method=\"q4_k_m\")\n",
        "\n",
        "# Create download package\n",
        "!zip -r fairness_chatbot_model.zip fairness_chatbot/\n",
        "\n",
        "print(\"ğŸ‰ Your fairness-focused chatbot is ready!\")\n",
        "print(\"ğŸ“Š Trained on 42 fairness & politeness examples\")\n",
        "print(\"ğŸ¯ Core values: Fair treatment, respectful communication, empathy\")\n",
        "print(\"ğŸ”„ Ready for persona integration while maintaining ethical foundation\")\n",
        "print(\"ğŸ“¥ Downloading...\")\n",
        "\n",
        "from google.colab import files\n",
        "files.download('fairness_chatbot_model.zip')\n",
        "print(\"âœ… Download complete!\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
