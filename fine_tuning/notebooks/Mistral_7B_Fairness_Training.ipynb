{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "# ğŸš€ Mistral 7B Fairness Training\n",
        "\n",
        "**Fast, reliable, and excellent performance for fairness!**\n",
        "\n",
        "## ğŸ¯ **Why Mistral 7B?**\n",
        "- **Fast Inference**: 2-3x faster than 24B models\n",
        "- **Reliable Training**: Stable on T4 GPU\n",
        "- **Excellent Performance**: Much better than GPT-2\n",
        "- **Easy Deployment**: Perfect for FastAPI servers\n",
        "- **No Authentication**: No HF token required\n",
        "\n",
        "## ğŸ“‹ **Simple Process:**\n",
        "- **Cell 1**: Setup & Install packages\n",
        "- **Cell 2**: Upload training data\n",
        "- **Cell 3**: Load Mistral 7B model\n",
        "- **Cell 4**: Prepare data & setup training\n",
        "- **Cell 5**: Train the model\n",
        "- **Cell 6**: Test & download\n",
        "\n",
        "**Total time: ~15-20 minutes**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ğŸ”§ Cell 1: Setup & Install Packages\n",
        "print(\"ğŸ”§ Setting up environment for Mistral 7B training...\")\n",
        "\n",
        "# Install packages\n",
        "!pip install -q torch==2.1.0 torchvision==0.16.0 torchaudio==2.1.0 --index-url https://download.pytorch.org/whl/cu118\n",
        "!pip install -q transformers>=4.36.0 datasets>=2.14.0 peft>=0.7.0 accelerate>=0.24.0 bitsandbytes>=0.41.0\n",
        "!pip install -q huggingface_hub\n",
        "\n",
        "# Import packages\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "from datasets import Dataset\n",
        "from google.colab import files\n",
        "from huggingface_hub import login, notebook_login\n",
        "import json\n",
        "import shutil\n",
        "\n",
        "print(\"âœ… Packages installed and imported!\")\n",
        "\n",
        "# ğŸ” Hugging Face Login\n",
        "print(\"ğŸ” Logging into Hugging Face...\")\n",
        "print(\"ğŸ“‹ This is required to access the Mistral model\")\n",
        "print(\"â„¹ï¸  Get your token from: https://huggingface.co/settings/tokens\")\n",
        "\n",
        "try:\n",
        "    # Use notebook_login for interactive login in Colab\n",
        "    notebook_login()\n",
        "    print(\"âœ… Successfully logged into Hugging Face!\")\n",
        "except Exception as e:\n",
        "    print(f\"âŒ Login failed: {e}\")\n",
        "    print(\"ğŸ’¡ Please:\")\n",
        "    print(\"   1. Go to https://huggingface.co/settings/tokens\")\n",
        "    print(\"   2. Create a token (Read access is enough)\")\n",
        "    print(\"   3. Run this cell again\")\n",
        "    raise\n",
        "\n",
        "# Check GPU\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"ğŸ”¥ GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"ğŸ’¾ GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
        "else:\n",
        "    print(\"âš ï¸ No GPU detected - training will be slower\")\n",
        "\n",
        "print(\"ğŸš€ Ready for Mistral 7B training!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ğŸ“¤ Cell 2: Upload Training Data\n",
        "print(\"ğŸ“¤ Upload your fairness_politeness_training_v2.jsonl file:\")\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "if 'fairness_politeness_training_v2.jsonl' not in uploaded:\n",
        "    print(\"âŒ Please upload the training file and run this cell again\")\n",
        "else:\n",
        "    # Preview the data\n",
        "    with open('fairness_politeness_training_v2.jsonl', 'r') as f:\n",
        "        lines = f.readlines()\n",
        "        print(f\"âœ… Training data uploaded! Dataset contains {len(lines)} examples\")\n",
        "        \n",
        "        # Show first example\n",
        "        first_example = json.loads(lines[0])\n",
        "        print(f\"\\nğŸ” First example preview:\\n{first_example['text'][:200]}...\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ğŸ¤– Cell 3: Load Mistral 7B Model\n",
        "print(\"ğŸ¤– Loading Mistral 7B model...\")\n",
        "\n",
        "model_name = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
        "print(f\"ğŸ“‹ Model: {model_name}\")\n",
        "print(\"ğŸ¯ Fast, reliable, and excellent for fairness!\")\n",
        "\n",
        "# Load tokenizer\n",
        "print(\"ğŸ”„ Loading tokenizer...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "print(\"âœ… Tokenizer loaded!\")\n",
        "\n",
        "# Load model with 4-bit quantization\n",
        "print(\"ğŸ”„ Loading model with 4-bit quantization...\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\",\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")\n",
        "\n",
        "print(\"âœ… Mistral 7B loaded successfully!\")\n",
        "print(f\"ğŸ“Š Model size: 7B parameters\")\n",
        "print(f\"ğŸ”¥ Device: {next(model.parameters()).device}\")\n",
        "print(f\"âš¡ Optimized for fast inference!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ğŸ“‹ Cell 4: Prepare Data & Setup Training\n",
        "print(\"ğŸ“‹ Preparing training data and LoRA setup...\")\n",
        "\n",
        "# Format data for Mistral 7B\n",
        "def format_data_for_mistral(file_path):\n",
        "    data = []\n",
        "    with open(file_path, 'r') as f:\n",
        "        for line in f:\n",
        "            item = json.loads(line.strip())\n",
        "            text = item['text']\n",
        "            if 'User:' in text and 'Assistant:' in text:\n",
        "                user_part = text.split('Assistant:')[0].replace('User:', '').strip()\n",
        "                assistant_part = text.split('Assistant:')[1].strip()\n",
        "                formatted = f\"[INST]{user_part}[/INST]{assistant_part}</s>\"\n",
        "                data.append({\"text\": formatted})\n",
        "    return data\n",
        "\n",
        "# Format and tokenize data\n",
        "train_data = format_data_for_mistral('fairness_politeness_training_v2.jsonl')\n",
        "dataset = Dataset.from_list(train_data)\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"text\"], truncation=True, padding=False, max_length=512)\n",
        "\n",
        "tokenized_dataset = dataset.map(tokenize_function, batched=True, remove_columns=dataset.column_names)\n",
        "\n",
        "print(f\"âœ… Formatted {len(train_data)} examples\")\n",
        "print(f\"ğŸ” Example: {train_data[0]['text'][:150]}...\")\n",
        "\n",
        "# Setup LoRA\n",
        "lora_config = LoraConfig(\n",
        "    task_type=TaskType.CAUSAL_LM,\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.1,\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    bias=\"none\",\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, lora_config)\n",
        "\n",
        "# Print trainable parameters\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "\n",
        "print(f\"âœ… LoRA applied!\")\n",
        "print(f\"ğŸ“Š Trainable: {trainable_params:,} ({100 * trainable_params / total_params:.2f}%)\")\n",
        "\n",
        "# Training setup\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./mistral_7b_fairness\",\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=2,\n",
        "    gradient_accumulation_steps=4,\n",
        "    learning_rate=2e-4,\n",
        "    fp16=True,\n",
        "    logging_steps=5,\n",
        "    save_strategy=\"epoch\",\n",
        "    warmup_steps=10,\n",
        ")\n",
        "\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
        "\n",
        "print(\"âœ… Training setup complete!\")\n",
        "print(f\"ğŸ“Š Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ğŸš€ Cell 5: Train the Model\n",
        "print(\"ğŸš€ Starting Mistral 7B fairness training...\")\n",
        "print(\"â±ï¸ Expected time: ~15-20 minutes\")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset,\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "# Start training\n",
        "print(\"ğŸ”¥ Training started!\")\n",
        "trainer.train()\n",
        "\n",
        "print(\"âœ… Training completed!\")\n",
        "print(\"ğŸ¯ Mistral 7B is now trained for fairness!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ğŸ§ª Cell 6: Test & Download Model\n",
        "print(\"ğŸ§ª Testing the trained Mistral 7B model...\")\n",
        "\n",
        "# Test the model\n",
        "def test_model(prompt):\n",
        "    formatted_prompt = f\"[INST]{prompt}[/INST]\"\n",
        "    inputs = tokenizer(formatted_prompt, return_tensors=\"pt\").to(model.device)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=150,\n",
        "            temperature=0.7,\n",
        "            do_sample=True,\n",
        "            pad_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "    \n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return response.split(\"[/INST]\")[-1].strip()\n",
        "\n",
        "# Test with fairness scenarios\n",
        "test_prompts = [\n",
        "    \"How should I handle a situation where someone is being discriminated against?\",\n",
        "    \"What's the best way to promote equality in the workplace?\",\n",
        "    \"How can I be more inclusive in my daily interactions?\"\n",
        "]\n",
        "\n",
        "print(\"ğŸ” Testing fairness responses:\")\n",
        "for i, prompt in enumerate(test_prompts, 1):\n",
        "    print(f\"\\n--- Test {i} ---\")\n",
        "    print(f\"User: {prompt}\")\n",
        "    response = test_model(prompt)\n",
        "    print(f\"Assistant: {response}\")\n",
        "\n",
        "# Save and download the model\n",
        "print(\"\\nğŸ’¾ Saving trained model...\")\n",
        "model.save_pretrained(\"./mistral_7b_fairness_final\")\n",
        "tokenizer.save_pretrained(\"./mistral_7b_fairness_final\")\n",
        "\n",
        "# Create a zip file for download\n",
        "print(\"ğŸ“¦ Creating download package...\")\n",
        "shutil.make_archive(\"mistral_7b_fairness_model\", 'zip', \"./mistral_7b_fairness_final\")\n",
        "\n",
        "print(\"â¬‡ï¸ Downloading trained model...\")\n",
        "files.download(\"mistral_7b_fairness_model.zip\")\n",
        "\n",
        "print(\"âœ… All done! Your Mistral 7B fairness model is ready!\")\n",
        "print(\"ğŸš€ Fast inference, excellent fairness, perfect for your chatbot!\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
