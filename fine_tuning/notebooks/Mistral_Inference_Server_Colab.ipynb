{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 🚀 Mistral Fairness Model - Cloud Inference Server\n",
        "\n",
        "Run your trained Mistral model inference server in Google Colab with GPU acceleration!\n",
        "\n",
        "**Features:**\n",
        "- FastAPI server with GPU acceleration\n",
        "- Public URL via ngrok for external access\n",
        "- Optimized for fairness and politeness responses\n",
        "- Direct integration with your Next.js app\n",
        "\n",
        "**Setup Steps:**\n",
        "1. Upload your trained model to Google Drive\n",
        "2. Run all cells below\n",
        "3. Copy the ngrok URL and update your Next.js app\n",
        "4. Start chatting with your trained model!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 🔧 Setup Environment\n",
        "!pip install fastapi uvicorn[standard] transformers torch peft accelerate bitsandbytes\n",
        "!pip install pyngrok\n",
        "\n",
        "# Mount Google Drive to access your trained model\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 🌐 Setup ngrok for public access\n",
        "from pyngrok import ngrok\n",
        "import getpass\n",
        "\n",
        "# Get your ngrok authtoken (free account at https://ngrok.com)\n",
        "print(\"Get your free ngrok authtoken at: https://dashboard.ngrok.com/get-started/your-authtoken\")\n",
        "ngrok_token = getpass.getpass(\"Enter your ngrok authtoken: \")\n",
        "ngrok.set_auth_token(ngrok_token)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 🤖 FastAPI Inference Server (GPU Optimized)\n",
        "from fastapi import FastAPI, HTTPException\n",
        "from fastapi.middleware.cors import CORSMiddleware\n",
        "from pydantic import BaseModel\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from peft import PeftModel\n",
        "import uvicorn\n",
        "from typing import Optional\n",
        "import logging\n",
        "import time\n",
        "import threading\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "app = FastAPI(\n",
        "    title=\"Fairness ChatBot API - Cloud\",\n",
        "    description=\"AI model trained for fairness and politeness - Running on GPU!\",\n",
        "    version=\"1.0.0\"\n",
        ")\n",
        "\n",
        "# Enable CORS for all origins (since we're using ngrok)\n",
        "app.add_middleware(\n",
        "    CORSMiddleware,\n",
        "    allow_origins=[\"*\"],  # Allow all origins for ngrok\n",
        "    allow_credentials=True,\n",
        "    allow_methods=[\"*\"],\n",
        "    allow_headers=[\"*\"],\n",
        ")\n",
        "\n",
        "# Request/Response models\n",
        "class ChatRequest(BaseModel):\n",
        "    message: str\n",
        "    persona: Optional[str] = \"\"\n",
        "    temperature: Optional[float] = 0.7\n",
        "    max_length: Optional[int] = 50\n",
        "\n",
        "class ChatResponse(BaseModel):\n",
        "    response: str\n",
        "    model: str\n",
        "    fairness_enabled: bool\n",
        "    processing_time_ms: int\n",
        "    gpu_used: bool\n",
        "\n",
        "class HealthResponse(BaseModel):\n",
        "    status: str\n",
        "    model_loaded: bool\n",
        "    gpu_available: bool\n",
        "    device: str\n",
        "\n",
        "class CloudFairnessChatBot:\n",
        "    def __init__(self, model_path: str):\n",
        "        \"\"\"Load the fairness-trained Mistral model with GPU acceleration\"\"\"\n",
        "        logger.info(f\"Loading fairness Mistral model from {model_path}...\")\n",
        "        \n",
        "        # Check GPU availability\n",
        "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        logger.info(f\"Using device: {self.device}\")\n",
        "        \n",
        "        try:\n",
        "            base_model_name = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
        "            \n",
        "            logger.info(f\"Loading base model: {base_model_name}\")\n",
        "            base_model = AutoModelForCausalLM.from_pretrained(\n",
        "                base_model_name,\n",
        "                torch_dtype=torch.float16 if self.device == \"cuda\" else torch.float32,\n",
        "                low_cpu_mem_usage=True,\n",
        "                device_map=\"auto\" if self.device == \"cuda\" else \"cpu\",\n",
        "                load_in_4bit=True if self.device == \"cuda\" else False,\n",
        "                trust_remote_code=True\n",
        "            )\n",
        "            \n",
        "            logger.info(f\"Loading LoRA adapters from: {model_path}\")\n",
        "            self.model = PeftModel.from_pretrained(base_model, model_path)\n",
        "            self.tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
        "            \n",
        "            if self.tokenizer.pad_token is None:\n",
        "                self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "            \n",
        "            self.model.eval()\n",
        "            \n",
        "            if self.device == \"cuda\":\n",
        "                self.model = self.model.cuda()\n",
        "            \n",
        "            logger.info(f\"✅ Fairness Mistral model loaded successfully on {self.device}!\")\n",
        "            \n",
        "        except Exception as e:\n",
        "            logger.error(f\"Failed to load model: {e}\")\n",
        "            raise e\n",
        "    \n",
        "    def generate_response(self, prompt: str, persona_context: str = \"\", max_length: int = 50, temperature: float = 0.7) -> str:\n",
        "        \"\"\"Generate a fair and polite response with GPU acceleration\"\"\"\n",
        "        # Format prompt for Mistral\n",
        "        if persona_context.strip():\n",
        "            persona_lines = persona_context.split('\\n')\n",
        "            persona_role = \"Assistant\"  # Default\n",
        "            for line in persona_lines:\n",
        "                if \"You are\" in line or \"Act as\" in line:\n",
        "                    persona_role = line.replace(\"You are\", \"\").replace(\"Act as\", \"\").strip()\n",
        "                    break\n",
        "            \n",
        "            formatted_prompt = f\"[INST]As a {persona_role}, respond politely and fairly to: {prompt}[/INST]\"\n",
        "        else:\n",
        "            formatted_prompt = f\"[INST]Respond politely and fairly to: {prompt}[/INST]\"\n",
        "        \n",
        "        inputs = self.tokenizer.encode(formatted_prompt, return_tensors=\"pt\")\n",
        "        if self.device == \"cuda\":\n",
        "            inputs = inputs.cuda()\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            outputs = self.model.generate(\n",
        "                inputs,\n",
        "                max_length=len(inputs[0]) + max_length,\n",
        "                temperature=temperature,\n",
        "                do_sample=True,\n",
        "                top_p=0.9,\n",
        "                top_k=50,\n",
        "                pad_token_id=self.tokenizer.eos_token_id,\n",
        "                eos_token_id=self.tokenizer.eos_token_id,\n",
        "                repetition_penalty=1.1,\n",
        "                no_repeat_ngram_size=3,\n",
        "                early_stopping=True\n",
        "            )\n",
        "        \n",
        "        response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        response = response.replace(formatted_prompt, \"\").strip()\n",
        "        \n",
        "        # Clean up response\n",
        "        response = response.split(\"\\nHuman:\")[0].strip()\n",
        "        response = response.split(\"\\nAssistant:\")[0].strip()\n",
        "        \n",
        "        return response\n",
        "\n",
        "# Global bot instance\n",
        "bot = None\n",
        "\n",
        "@app.post(\"/chat\", response_model=ChatResponse)\n",
        "async def chat(request: ChatRequest):\n",
        "    if bot is None:\n",
        "        raise HTTPException(status_code=503, detail=\"Model not loaded\")\n",
        "    \n",
        "    start_time = time.time()\n",
        "    \n",
        "    try:\n",
        "        response = bot.generate_response(\n",
        "            request.message,\n",
        "            request.persona,\n",
        "            request.max_length,\n",
        "            request.temperature\n",
        "        )\n",
        "        \n",
        "        processing_time = int((time.time() - start_time) * 1000)\n",
        "        \n",
        "        return ChatResponse(\n",
        "            response=response,\n",
        "            model=\"mistral-7b-fairness\",\n",
        "            fairness_enabled=True,\n",
        "            processing_time_ms=processing_time,\n",
        "            gpu_used=torch.cuda.is_available()\n",
        "        )\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error generating response: {e}\")\n",
        "        raise HTTPException(status_code=500, detail=str(e))\n",
        "\n",
        "@app.get(\"/health\", response_model=HealthResponse)\n",
        "async def health():\n",
        "    return HealthResponse(\n",
        "        status=\"healthy\" if bot is not None else \"loading\",\n",
        "        model_loaded=bot is not None,\n",
        "        gpu_available=torch.cuda.is_available(),\n",
        "        device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    )\n",
        "\n",
        "@app.get(\"/\")\n",
        "async def root():\n",
        "    return {\"message\": \"Fairness ChatBot API - Cloud Edition\", \"gpu_available\": torch.cuda.is_available()}\n",
        "\n",
        "print(\"✅ FastAPI server code loaded!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 📁 Load Your Trained Model\n",
        "# Update this path to point to your trained model in Google Drive\n",
        "MODEL_PATH = \"/content/drive/MyDrive/chatbot_models/mistral_fairness_model\"  # Update this path!\n",
        "\n",
        "print(f\"Loading model from: {MODEL_PATH}\")\n",
        "try:\n",
        "    bot = CloudFairnessChatBot(MODEL_PATH)\n",
        "    print(\"✅ Model loaded successfully!\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ Error loading model: {e}\")\n",
        "    print(\"\\n📝 Make sure to:\")\n",
        "    print(\"1. Upload your trained model to Google Drive\")\n",
        "    print(\"2. Update the MODEL_PATH variable above\")\n",
        "    print(\"3. Re-run this cell\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 🧪 Test Your Model\n",
        "if bot is not None:\n",
        "    test_message = \"You people are always so slow and incompetent!\"\n",
        "    test_persona = \"You are a professional hotel receptionist.\"\n",
        "    \n",
        "    print(f\"Test Input: {test_message}\")\n",
        "    print(f\"Persona: {test_persona}\")\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    \n",
        "    response = bot.generate_response(test_message, test_persona)\n",
        "    print(f\"Model Response: {response}\")\n",
        "    print(\"\\n✅ Model is working correctly!\")\n",
        "else:\n",
        "    print(\"❌ Model not loaded. Please run the previous cell first.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 🚀 Start the FastAPI Server with ngrok\n",
        "import nest_asyncio\n",
        "import threading\n",
        "import time\n",
        "\n",
        "nest_asyncio.apply()\n",
        "\n",
        "def run_server():\n",
        "    uvicorn.run(app, host=\"0.0.0.0\", port=8000, log_level=\"info\")\n",
        "\n",
        "# Start server in background thread\n",
        "server_thread = threading.Thread(target=run_server, daemon=True)\n",
        "server_thread.start()\n",
        "\n",
        "# Wait for server to start\n",
        "time.sleep(5)\n",
        "\n",
        "# Create ngrok tunnel\n",
        "public_url = ngrok.connect(8000)\n",
        "print(f\"\\n🌐 Your FastAPI server is now running at:\")\n",
        "print(f\"Public URL: {public_url}\")\n",
        "print(f\"\\n📋 API Endpoints:\")\n",
        "print(f\"• Health Check: {public_url}/health\")\n",
        "print(f\"• Chat: {public_url}/chat (POST)\")\n",
        "print(f\"• API Docs: {public_url}/docs\")\n",
        "print(f\"\\n🔗 Update your Next.js app to use: {public_url}\")\n",
        "print(f\"\\n⚡ GPU Acceleration: {'✅ Enabled' if torch.cuda.is_available() else '❌ CPU Only'}\")\n",
        "print(f\"\\n🎯 Your trained fairness model is now accessible from anywhere!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 🧪 Test the API via HTTP\n",
        "import requests\n",
        "import json\n",
        "\n",
        "# Get the current ngrok URL\n",
        "tunnels = ngrok.get_tunnels()\n",
        "if tunnels:\n",
        "    api_url = str(tunnels[0].public_url)\n",
        "    \n",
        "    # Test health endpoint\n",
        "    health_response = requests.get(f\"{api_url}/health\")\n",
        "    print(\"Health Check:\")\n",
        "    print(json.dumps(health_response.json(), indent=2))\n",
        "    \n",
        "    # Test chat endpoint\n",
        "    chat_data = {\n",
        "        \"message\": \"This service is terrible and you people don't know what you're doing!\",\n",
        "        \"persona\": \"You are a professional customer service representative.\",\n",
        "        \"temperature\": 0.7,\n",
        "        \"max_length\": 50\n",
        "    }\n",
        "    \n",
        "    chat_response = requests.post(f\"{api_url}/chat\", json=chat_data)\n",
        "    print(\"\\nChat Response:\")\n",
        "    print(json.dumps(chat_response.json(), indent=2))\n",
        "    \n",
        "    print(f\"\\n✅ API is working! Use this URL in your Next.js app: {api_url}\")\n",
        "else:\n",
        "    print(\"❌ No ngrok tunnels found. Please run the previous cell first.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## 🔗 Integration with Your Next.js App\n",
        "\n",
        "Update your Next.js API route to use the ngrok URL:\n",
        "\n",
        "```typescript\n",
        "// app/api/trained-model/route.ts\n",
        "const CLOUD_MODEL_URL = \"YOUR_NGROK_URL_HERE\"; // Copy from above\n",
        "\n",
        "export async function POST(request: Request) {\n",
        "  try {\n",
        "    const { message, persona, temperature = 0.7, max_length = 50 } = await request.json();\n",
        "    \n",
        "    const response = await fetch(`${CLOUD_MODEL_URL}/chat`, {\n",
        "      method: 'POST',\n",
        "      headers: {\n",
        "        'Content-Type': 'application/json',\n",
        "      },\n",
        "      body: JSON.stringify({\n",
        "        message,\n",
        "        persona,\n",
        "        temperature,\n",
        "        max_length\n",
        "      })\n",
        "    });\n",
        "    \n",
        "    const data = await response.json();\n",
        "    return Response.json(data);\n",
        "  } catch (error) {\n",
        "    return Response.json(\n",
        "      { error: 'Failed to get response from trained model' },\n",
        "      { status: 500 }\n",
        "    );\n",
        "  }\n",
        "}\n",
        "```\n",
        "\n",
        "## 📝 Notes\n",
        "\n",
        "- **Free GPU**: Google Colab provides free GPU access (T4)\n",
        "- **Session Limits**: Free tier has ~12 hours of continuous usage\n",
        "- **Persistent URLs**: ngrok URLs change each time you restart\n",
        "- **Performance**: GPU inference is ~10-20x faster than CPU\n",
        "- **Scaling**: For production, consider upgrading to Colab Pro or using cloud platforms\n",
        "\n",
        "## 🔄 Keeping It Running\n",
        "\n",
        "The server will keep running as long as this Colab session is active. To keep it running longer:\n",
        "1. Keep the browser tab open\n",
        "2. Interact with the notebook occasionally\n",
        "3. Consider Colab Pro for longer sessions\n",
        "4. For production, deploy to a proper cloud service\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
